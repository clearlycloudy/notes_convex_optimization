\documentclass[12pt,letter]{article}

%% \usepackage[fleqn]{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{algorithm2e}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subcaption}
%% \usepackage{datetime}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{parskip} %turns off paragraph indent
\pagestyle{fancy}

\usetikzlibrary{arrows}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}

\newcommand{\mathleft}{\@fleqntrue\@mathmargin0pt}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}} 
\newcommand{\N}{\mathbb{N}}

\setcounter{MaxMatrixCols}{20}

% remove excess vertical space for align equations
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\begin {document}

\lhead{Notes - Convex Optimization, 2020/01/15}

\begin{enumerate}
\item matrix decomposition\\
  singular value decomposition:\\
  $A=U\Sigma V^T$\\
  $Ax=U\Sigma V^Tx$
\item eigenvalue decomposition\\
  For $A=A^T$:\\
  Let $S^n$ be the set of real symmetric metrices of size n.\\
  $Av=\lambda v \implies \lambda$ is a eigenvalue of A, $v$ is eigenvector of A\\
  $A=Q\Sigma Q^T$, $Q$ is a matrix with columns of eigenvectors\\
\item real symmetric matrices have real eigenvalues:\\
  proof:
  \begin{align*}
    Av&=\lambda v\\ 
    v^*Av&=v^*\lambda v=\lambda \|v\|_2^2\\
    (v^* A v)^*&= v^* A^* v = \lambda^* \|v\|_2^2\\
      &\implies \lambda = \lambda^*
    \end{align*}
\item positive (semi)-definite: $A\geq 0$\\
  $A$ real symmetric, $(\forall v) v^TAv \geq 0 \implies A$  PSD\\
  $A$ real symmetric, $(\forall v) v^TAv > 0 \implies A$ PD\\
\item $A \in S_+^n$ iff all eigenvalues are non-negative\\
  proof:\\
  \begin{align*}
    v^T A V &= v^T Q \Sigma Q^T v\\
    v^T A V &= \tilde{v} Q \Sigma \tilde{v}=\sum \lambda_i \|v_i\|^2\\
    (\forall i) \lambda_i \geq 0) & \implies (\forall v)v^T A v \geq 0
  \end{align*}
\item for PSD matrices, we can take square root\\
  $A^{\frac{1}{2}}=Q\Sigma^{\frac{1}{2}} Q^T$, $A^{\frac{1}{2}}A^{\frac{1}{2}}=A=Q\Sigma Q^T$\\

  \pagebreak
  
\item functions of matrices $f:S_{++}^n \to \R$\\
  how to compute gradient\\
  $f(x)=log(det X)$\\
  \begin{align*}
    f(X+\delta X) &= logdet(X+\delta X)\\
                  &= logdet(X^{\frac{1}{2}}(I+X^{-\frac{1}{2}}) \delta X X^{-\frac{1}{2}})X^{\frac{1}{2}})\\
                  &= logdet(X)+logdet(I+X^{\frac{1}{2}} \delta X X^{-\frac{1}{2}})\\
                  & \text{let } M = X^{\frac{1}{2}} \delta X X^{-\frac{1}{2}}\\
                  &= logdet(X)+logdet(I+M)
  \end{align*}
  claim eigenvalues of $I+M$: $1+\lambda_i$
  \begin{align*}
    Mv_i &= \lambda_i v_i\\
    (I+M)v_i &= (1+\lambda_i)v_i\\
    det(M)&=\prod_i(1+\lambda_i)\\
    f(X+\delta X) &= logdet(X) + log \prod_i(1+\lambda_i)\\
         &= logdet(X) + \sum_i log(1+\lambda_i)\\
         &\approx logdet(X) + \sum_i \lambda_i\\
         &\approx logdet(X) + trace(X^{-\frac{1}{2}} \delta X X^{\frac{1}{2}})\\
         &\approx logdet(X) + trace(X^{-1} \delta X)\\
    trace(X^{-1} \delta X) &= <X^{-T},\delta X>\\
    f(X+\delta X) &= f(X) + <\nabla f(X), \delta X> \implies \nabla f(X) = X^{-1}\\
    logdet(X) &= log(X) \implies f'(X)=\frac{1}{X}
  \end{align*}

  \pagebreak
  
\item 2nd order approximation
  \begin{align*}
    f(X+\delta X) = f(X) + <\nabla f(X), \delta X>  + 1/2 <\delta X, \nabla^2 f(x) \delta X>
  \end{align*}
  first look at first order approximation: $g(X) = X^{-1}$
  \begin{align*}
    g(X+\delta X) &= (X+\delta X)^{-1} = (X^{\frac{1}{2}}(I+X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}}) X^{\frac{1}{2}})^{-1}\\
                  &= X^{-\frac{1}{2}}(I+X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}})^{-1} X^{-\frac{1}{2}}\\
                  &\text{for small A(small eigenvalues): } (I+A)^{-1} \approx I-A\\
                  &= X^{-\frac{1}{2}}(I-X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}}) X^{-\frac{1}{2}}\\
                  &= X^{-1} - X^{-1} \delta X X^{-1}
  \end{align*}
  \begin{align*}
    logdet(X+\delta X)&=logdet(X)+tr(X^{-1}\delta X) - \frac{1}{2} tr(\delta X X^{-1} \delta X X^{-1})
  \end{align*}
  analogy:
  \begin{align*}
    g(x)&=\frac{1}{X}\\
    g'(x)&=\frac{-1}{X^2} = -X^{-2}
  \end{align*}
\item convex sets\\
  affine sets: a set $C \subseteq \R^n$ is affine if $(\forall x_1, x_2 \leq C)(\forall \theta \in \R) \implies \theta x_1 +(1-\theta) x_2 \in C$\\
  can use this definition to check sets for convexity property\\
  eg: set of solution to a set of linear equations is affine\\
  affine combination: $\sum_i \theta_i x_i, \forall \theta_i \in \R, \sum_i \theta_i = 1$\\
\item convex sets\\
  a set $C \subseteq \R^n$ is convex if $(\forall x_1,x_2 \in C)(\forall \theta \in \R) 0 \leq \theta \leq 1 \implies \theta x_1 +(1-\theta)x_2 \in C$\\
  convex combination: $\sum_i \theta_i x_i, \forall \theta_i \in \R, \sum_i \theta_i = 1, \theta_i \geq 0$\\
  convex hull: the set of all convex combinations of points in $C$, the hull is convex\\
  any affine set is convex, since convex property is a constrained version of the affine property\\
\item hyperplane
  \begin{align*}
    C=\{x: a^Tx=b\}, a \in \R^n, a \neq 0, b \in \R\\
  \end{align*}
\item halfspaces
  \begin{align*}
    C&=\{x: a^Tx \leq b\}, a \in \R^n, a \neq 0, b \in \R\\
     &\text{let } a^Tx_c=b\\
     &=\{x: a^T(x-x_c) \leq 0\}, a \in \R^n, a \neq 0
    \end{align*}
\end{enumerate}

\end {document}
