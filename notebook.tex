\documentclass[8pt]{report}

%% \usepackage[fleqn]{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{algorithm2e}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subcaption}
%% \usepackage{datetime}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{parskip} %turns off paragraph indent
\pagestyle{fancy}

\usetikzlibrary{arrows}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}

\newcommand{\mathleft}{\@fleqntrue\@mathmargin0pt}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}} 
\newcommand{\N}{\mathbb{N}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\ppartial}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\set}[1]{\{#1\}}

\setcounter{MaxMatrixCols}{20}

% remove excess vertical space for align* equations
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\usepackage{multicol}

\usepackage[tiny]{titlesec}

\begin{document}

\lhead{Notebook - Convex Optimization}

\begin{multicols*}{2}

  \section{Symbols}

  $S^n \equiv$ set of all symmetric matrices\\
  
  $M_+^n \equiv$ set of all positive definite matrices (PD)\\
  
  $S_+^n \equiv$ set of all symmetric positive semidefinite matrices (SPSD)\\
  
  $S_++^n \equiv$ set of all symmetric positive definite matrices (SPD)\\
  
  $cl(X) \equiv$ closure of $X$\\

  $relint(X) \equiv$ relative interior of $X$\\
  
  $int(X) \equiv$ interior of $X$\\
  
  \vfill\null
  \columnbreak
  \vfill\null
  \columnbreak
  
  \section{Preliminary}
  
  Consider $f:\R^n \to \R$\\
  Gradient of $f$: $\nabla f(x) = \begin{bmatrix} \partial f / \partial x_i \\ .. \end{bmatrix}$\\
  $f(x) = a^Tx \implies \nabla f(x) = a$\\
  $f(x) = x^TPx, P=P^T \implies \nabla f(x) = 2Px$\\
  $f(x) = x^TPx \implies \nabla f(x) = 2(\frac{P^T+P}{2})x=(P^T+P)x$\\

  Taylor expansion approximation:\\
  $f(x) \approx f(x_0) + \nabla^T f(x_0)(x-x_0) + o((x-x_0)^2)$\\
  $f(x+\delta x) \approx f(x_0) + \nabla^T f(x)\delta x + o((\delta x)^2)$\\
  
  Chain rule:\\
  $f: \R \to \R, g: \R \to \R, h(x) = f(g(x))$\\
  $\nabla h(x) = g'(f(x))  \nabla f(x)$\\

  $g:\R^m \to \R, g(x) = f(Ax+b)$\\
  $\nabla g(x) = A^T \nabla f(Ax+b)$\\

  2nd derivative:\\
  $\nabla^2 f(x)=\begin{bmatrix}
    \partial^2 f / \partial x_1 \partial x_1 & ...\\
    .. & \partial^2 f / \partial x_n \partial x_n
  \end{bmatrix}$\\
  $\nabla f(x)=Px+g$\\
  $\nabla^2 f(x)=P$\\

  Hessian gives the 2nd order approximation:\\
  $f(x) \approx f(x_0) + \nabla^T f(x_0)(x-x_0) + \\ \frac{1}{2}(x-x_0)^T \nabla^2 f(x_0) (x-x_0)$\\

  Matrices:\\
  $A \in \R^{m \times n}$: set of all real matrices\\
  inner product: $\sum_i \sum_j x_{ij} y_{ij} = trace(XY^T)=trace(Y^TX)=\sum_{i}(XY)_{ii}$\\
  note trace has cyclic property\\
  frobenius norm: $\|X\|_F  = (\sum_i \sum_j X_{ij}^2)^{\frac{1}{2}}$\\
  range: $R(A) = \{Ax: x \in \R^n\}=\sum_i a_i x_i$, where $a_i$ is ith column (column space of A)\\
  null space: $N(A) = \{ x : Ax = 0\}$\\

  SVD:\\
  $A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V_{n \times n}^T$\\
  U and V are left and right eigenvector matrixes\\
  U and V are orthogonal matrixes($BB^T=B^TB=I$)\\
  $\Sigma$ is rectangular diagonal matrix of eigenvalues\\
  $A_{m \times n}x_{n}$\\
  linear transformation: $U \Sigma V^T x$\\
  rotation - scaling - rotation
  \vfill\null
  \columnbreak
  
  PSD matrix:\\
  $A\ PSD \iff (\forall x) x^TAx \geq 0 \iff (\forall i) \lambda_i(A) \geq 0$\\
  $A\ PSD \implies A^{1/2}$ exists\\

  Real symmetric matrices have real eigenvalues:
  \begin{align*}
    &Av=\lambda v\\ 
    &v^*Av=v^*\lambda v=\lambda \|v\|_2^2\\
    &(v^* A v)^*= v^* A^* v = \lambda^* \|v\|_2^2 \implies \lambda = \lambda^*
  \end{align*}

  Affine sets\\
  A set $C \subseteq \R^n$ is affine if $(\forall x_1, x_2 \in C)(\forall \theta \in \R) \implies \theta x_1 +(1-\theta) x_2 \in C$\\
  
  Convex sets\\
  A set $C \subseteq \R^n$ is convex if $(\forall x_1,x_2 \in C)(\forall \theta \in \R) 0 \leq \theta \leq 1 \implies \theta x_1 +(1-\theta)x_2 \in C$\\

  Operations preserving convex sets:
  \begin{itemize}
  \item partial sum
  \item sum
  \item coordinate projection
  \item scaling
  \item translation
  \item intersection between any convex sets
  \end{itemize}  

  Separating Hyperplanes: if $S,T \subset \R^n$ are convex and disjoint, then $\exists a \neq 0, b$ such that:\\
  \begin{align*}
    a^Tx \geq b, \forall x \in S\\
    a^Tx \leq b, \forall x \in T\\
  \end{align*}

  Supporting Hyperplane:\\
  if $S$ is convex, $\forall x_0 \in \partial S$ (boundary of S), then $\exists a \neq 0$ such that $a^Tx \leq a^Tx_0, \forall x \in S$\\
  
  Convex combination:\\
  $\sum_i \theta_i x_i, \forall \theta_i \in \R, \sum_i \theta_i = 1, \theta_i \geq 0$\\
  
  Convex hull:\\
  The set of all convex combinations of points in $C$, the hull is convex\\

  Hyperplane
  \begin{align*}
    C=\{x: a^Tx=b\}, a \in \R^n, a \neq 0, b \in \R
  \end{align*}
  Halfspaces
  \begin{align*}
    C&=\{x: a^Tx \leq b\}, a \in \R^n, a \neq 0, b \in \R\\
     &\text{let } a^Tx_c=b\\
    C&=\{x: a^T(x-x_c) \leq 0\}, a \in \R^n, a \neq 0
  \end{align*}
  
  Elipse
  \begin{align*}
    E(x_c,P) = \{ x: (x-x_c)^T P^{-1} (x-x_c) \leq 1 \}, P > 0\\
    P=r^2 I \implies \text{ Euclidean Ball }\\
    P=Q \begin{bmatrix}
      \lambda_1 & ..\\
      .. & \lambda_n\\
    \end{bmatrix}
    Q^T\\
    (x-x_c)^T (Q \begin{bmatrix}
      \lambda_1 & ..\\
      .. & \lambda_n\\
    \end{bmatrix}
    Q^T)^{-1}(x-x_c) \leq 1\\
    \tilde{x}^T \begin{bmatrix}
      \frac{1}{\lambda_1} & ..\\
      .. & \frac{1}{\lambda_n}\\
    \end{bmatrix}
    \tilde{x} \leq 1\\
    \tilde{x}^T \begin{bmatrix}
      \frac{1}{\lambda_1} & ..\\
      .. & \frac{1}{\lambda_n}\\
    \end{bmatrix}
    \tilde{x} = \frac{\tilde{x_1}^2}{\lambda_1}+..+\frac{\tilde{x_n}^2}{\lambda_n}\leq 1\\
    \text{volum of elipsoid proportional to } \sqrt{det(P)}=\sqrt{\Pi_i \lambda_i}
  \end{align*}
  
  \vfill\null
  \columnbreak
  \vfill\null
  \columnbreak
  
  \section{Problem Types}
  \subsection*{LP}
  standard, inequality, general forms
  \begin{align*}
    \min_x c^Tx\ s.t.:\\
         Ax = b\\
         x \succeq 0
  \end{align*}

  \begin{align*}
    \min_x c^Tx\ s.t.:\\
    Ax \preceq b
  \end{align*}

  \begin{align*}
    \min_x c^Tx+d\ s.t.:\\
    Gx \preceq h\\
    Ax = b
  \end{align*}
  
  \subsection*{QP}
  \begin{align*}
    \min_x \frac{1}{2} x^T P x + q^T x + r\ s.t.:\\
    Gx \leq h\\
    Ax = b
  \end{align*}
  \subsection*{QCQP}
  \begin{align*}
    \min_x \frac{1}{2}x^TP_0x + q_0^Tx + r_0, s.t.:\\
    \frac{1}{2}x^TP_ix + q_i^Tx + r_i \leq 0, \forall i\\
    Ax=b
  \end{align*}
  \subsection*{SOCP}
  \begin{align*}
    \min_x  f^T x\ s.t.:\\
    \norm{A_ix+b_i}_2 \leq c_i^Tx+d_i, \forall i\\
    Fx = g
  \end{align*}
  \begin{align*}
    (\forall i) b_i=0 \implies LP\\
    (\forall i) c_i=0 \implies QCQP
  \end{align*}
  \vfill\null
  \columnbreak
  \subsection*{GP}
  \begin{align*}
    \min_x  f_0(x)\ s.t.:\\
    f_i(x) \leq 1, \forall i\\
    h_i(x) = 1, \forall i\\
    f_i\ is\ a\ posynomial := \sum_i h_i\\
    h_i\ is\ a\ monomial := cx_1^{a_1}x_2^{a_2}.., c>0, a_i \in \R\\
  \end{align*}
  Use transform of objective and constraint functions:\\
  $y_i=log x_i, x_i=e^{y_i}$\\
  $\tilde{h_i}$ becomes exponential of affine function\\
  $\tilde{f_i} = log(f_i)$ becomes log sum exp (convex)\\
  If all constraints and objective are monomials, reduces to LP after transform.
  \subsection*{SDP}
  general, standard, inequality forms
  \begin{align*}
    \min_x\ c^Tx\ s.t.:\\
    LMI:\ \sum_i^n x_i F_i + G \preceq_K 0\\
    Ax=b\\
    x\in\R^n\\
    F_i,G \in S^m, K\in S_+^m
  \end{align*}
  \begin{align*}
    \min_X\ tr(CX) s.t.:\\
    tr(A_iX)=b_i, \forall i\\
    X \succeq 0
  \end{align*}
  \begin{align*}
    \min_x\ c^Tx\ s.t.:\\
    \sum_i^n x_i A_i \preceq_K B\\
    Ax=b\\
    B,A_i \in S^m, K\in S_+^m
  \end{align*}
  concatenating constraints:
  \begin{align*}
    F^(i)(x) = \sum_j x_j F_i^{(i)} + G^{(i)} \preceq 0\\
    Gx \preceq h\\
    \implies\\
    diag(Gx-h, F^{(1)}(x),..,F^{(m)}(x)) \preceq 0\\
  \end{align*}  
  if all matrices are diagonal, reduces to LP
  \pagebreak

  \section{Convex/Concave Functions}

  \begin{itemize}
  \item Affine
  \item Pointwise supremum of convex function
    \begin{itemize}
    \item distance to farthest point in a set
    \item support function of set
    \end{itemize}
  \item Norm
  \item Non-negative weighted sum of convex functions
  \end{itemize}
  
  \subsection{log det X, concave}
  \begin{align*}
    let\ X=Z+tV \succ 0\\
    f=log det(Z+tV)\\
    f=log det(Z^{-0.5}(I+tZ^{-0.5}VZ^{0.5})Z^{0.5})\\
    f=log (det(Z^{-0.5})det(I+tZ^{-0.5}VZ^{0.5})det(Z^{0.5}))\\
    f=log (det(Z^{0})det(I+tZ^{-0.5}VZ^{0.5}))\\
    f=log det(I+tZ^{-0.5}VZ^{0.5})\\
    f=log \Pi_i (1+\lambda_i t)\\
    f=\sum_i log(1+\lambda_i t)\\
    \frac{\partial f}{\partial t} = \sum_i \frac{\lambda_i}{1+\lambda_i t}\\
    \frac{\partial^2 f}{\partial t^2} = \sum_i \frac{-\lambda_i^2}{(1+\lambda_i t)^2} = -\sum_i \frac{\lambda_i^2}{(1+\lambda_i t)^2} \leq 0\\
    \nabla^2 f \leq 0 \iff f\ concave\\
  \end{align*}

  \vfill\null
  \columnbreak
    
  \subsection{log $\sum_i exp(x_i)$, convex}
  \begin{align*}
    \nabla^2 f = \frac{1}{(1^Tz)^2} (1^Tz diag(z) -zz^T)\\
    v^T zz^T v = det(v^T zz^T v) = det(vv^T zz^T)\\
    v^T zz^T v = \sum_j \sum_i z_j z_i v_j v_i\\
    v^T zz^T v = (\sum_j z_j z_j)(\sum_i z_i v_i)\\
    v^T zz^T v = (\sum_i z_i v_i)^2\\
    use\ Holder's\ Inequality:\\
    \|a\|_2^2 \|b\|_2^2 \geq |a^Tb|^2\\
    let\ a = z_i^{0.5}, b=v_iz_i^{0.5}\\
    1^Tz(\sum_i v_i^2 z_i)-(\sum_i z_i v_i)^2 \geq 0\\
    v^T\nabla^2f v = \frac{1}{(1^Tz)^2} \bigg(1^Tz(\sum_i v_i^2 z_i)-(\sum_i z_i v_i)^2\bigg) \geq 0\\
    \nabla^2f \geq 0 \iff f\ convex
  \end{align*}  
  
  \subsection{geometric mean on $R_{++}^n$, concave}
  \begin{align*}
    f=(\Pi_i x_i)^{\frac{1}{n}}\\
    \frac{\partial}{\partial x_i} f = \frac{1}{n}(\Pi_i x_i)^{\frac{1}{n}-1}\Pi_{j\not=i}x_j\\
    \frac{\partial^2}{\partial x_i^2} f = \frac{1}{n}(\frac{1}{n}-1)(\Pi_i x_i)^{\frac{1}{n}-2}(\Pi_{j\not=i}x_j)^2\\
    \frac{\partial^2}{\partial x_i^2} f = \frac{1}{n}(\frac{1}{n}-1)\frac{(\Pi_i x_i)^{\frac{1}{n}}}{x_i^2}\\
    \frac{\partial^2}{\partial x_i x_k} f = \frac{1}{n^2}\frac{(\Pi_i x_i)^{\frac{1}{n}}}{x_i x_k}, i\not=k\\
    \frac{\partial^2}{\partial x_i x_k} f = \frac{1}{n^2}\frac{(\Pi_i x_i)^{\frac{1}{n}}}{x_i x_k} -\delta_{ik} \frac{1}{n}\frac{(\Pi_i x_i)^{\frac{1}{n}}}{x_i^2}
    \\
    v^T \nabla^2 f v = \frac{-(\Pi_i x_i)^{\frac{1}{n}}}{n^2}(n \sum_i \frac{v_i^2}{x_i^2} - (\sum_i \frac{v_i}{x_i})^2)\\\
    apply\ Cauchy\ Schwartz\ Inequality:\\
    let\ a=\bold{1}, b_i = \frac{v_i}{x_i}\\
    \|\bold{1}\|_2^2 (\sum_i \frac{v_i^2}{x_i}) \geq (\sum_i \frac{v_i}{x_i})^2\\
    n \sum_i \frac{v_i^2}{x_i^2} - (\sum_i \frac{v_i}{x_i})^2 \geq 0\\
    v^T \nabla^2 f v \leq 0 \iff f\ concave
  \end{align*}

  \subsection{quadratic over linear, convex}
  \begin{align*}
    f(x,y) = \frac{h(x)}{g(y)}, g(y) \ linear, g(y) \in R_+\\
    \nabla^2 f = vv^T \ is\ PSD \iff f\ convex
  \end{align*}

  \vfill\null
  \columnbreak
  
  \section{Composition of functions}

  Mnemonic derivation from scalar composite function
  \begin{align*}
    f=h(g(x))\\
    f'=g'(x)h'(g(x))\\
    f''=g''(x)h'(g(x)) + (g'(x))^2 h''(g(x))\\
    \\
    \text{h convex \& non-decreasing, g convex } \implies \text{ f convex}\\
    h'' \geq 0, g''(x) \geq 0, h'(g(x)) \geq 0 \implies f'' \geq 0\\
    \\
    \text{h convex \& non-increasing, g concave } \implies \text{ f convex}\\
    h'' \geq 0, g''(x) \leq 0, h'(g(x)) \leq 0 \implies f'' \geq 0\\
    \\
    \text{h concave \& non-decreasing, g concave } \implies \text{ f concave}\\
    h'' \leq 0, g''(x) \leq 0, h'(g(x)) \geq 0 \implies f'' \leq 0\\
    \\
    \text{h concave \& non-increasing, g convex } \implies \text{ f concave}\\
    h'' \leq 0, g''(x) \geq 0, h'(g(x)) \leq 0 \implies f'' \leq 0\\
  \end{align*}

  \vfill\null
  
  \pagebreak
  
  \section{Convexity Preservation of Sets}
  \subsection{Intersection}
  \begin{align*}
    (\forall \alpha \in A) S_{\alpha}\text{ is convex cone} \implies\\
    \cap_{\alpha \in A} S_{\alpha} \text{ is convex cone}
  \end{align*}
  Any closed convex set can be represented by possibly infinitely many half spaces.\\
  
  \subsection{Affine functions}
  let $f(x)=Ax+b, f:\R^n \to \R^m$\\
  then if S is a convex set we have:
  \begin{itemize}
  \item project forward: $f(S) = \{ f(X) : X \in S \}$ is convex
  \item project back: $f^{-1}(S) = \{ X : f(X) \in S \}$ is convex
  \end{itemize}
  Example:
  \begin{align*}
    C = \{ y : y=Ax+b, \norm{x} \leq 1\}
  \end{align*}
  $\norm{x} \leq 1$ is convex, $Ax+b$ is affine $\implies$ C is convex\\
  Example:
  \begin{align*}
    C = \{ x : \norm{Ax+b} \leq 1 \}
  \end{align*}
    $\{y: \norm{y} \leq 1 \}$ is convex $\wedge$ $y$ is an affine function of $x \implies$ C is convex\\

  \vfill\null
  \pagebreak

  \section{Constraint Qualifications}

  \subsection{Slater's Constraint Qual.}
  Optimal solution is in relative interior: $x^* \in relint(S)$\\
  
  Inequalities $(\forall i)f_i(x)$ convex $\wedge\ f_i(x)<0 \implies$ Slater's constraint satisfied.\\
  
  Inequalities $(\forall i)f_i(x)$ affine $\implies (\forall i)f_i(x) \leq 0 \wedge (\exists i) f_i(x) < 0 \implies$ Slater's constraint satisfied.\\
  
  Achieving Slater's constraint implies 0 duality gap.
  \subsection{KKT}
  Assumes optimality achieved with 0 duality gap: $\nabla L(x^*,\lambda^*,v^*)=0$
  \begin{align*}
    &L(x^*,\lambda^*,v^*) = f_0(x^*) + \sum_i \lambda_i^* f_i(x^*) + \sum_i v_i h_i(x^*)\\
    &\nabla L(x^*,\lambda^*,v^*) = \nabla f_0(x^*) + \sum_i \lambda_i^* \nabla f_i(x^*) + \sum_i v_i \nabla h_i(x^*)
  \end{align*}
  We have the constraints:
  \begin{align*}
    &f_i(x^*) \leq 0\\
    &\lambda_i^* \geq 0\\
    &h_i(x^*)=0\\
    &\lambda_i^* f_i(x^*)=0\\
    &\nabla L(x^*,\lambda^*,v^*) = \nabla f_0(x^*) + \sum_i \lambda_i^* \nabla f_i(x^*) + \sum_i v_i \nabla h_i(x^*)
  \end{align*}

  Primal inequality constraints convex and equality constraints affine and KKT satisfied $\implies$ 0 duality gap with specified points for primal and dual. (sufficient).\\

  If Slater's constraint satisfied then the above is sufficient and necessary:\\
  Primal inequality constraints convex and equality constraints affine and KKT satisfied $\iff$ 0 duality gap with specified points for primal and dual.\\
  
  \vfill\null
  
  \pagebreak
  
  \section{Definitions}

  \subsection{Convex Function}
  \begin{align*}
    f(\theta x +(1-\theta) y) \leq \theta f(x) + (1-\theta) f(y), \forall \theta = [0,1]\\
  \end{align*}
  For convenience we sometimes define an extended value function:\\
  \begin{align*}
    \tilde{f}(x) = \begin{cases}
      f(x), & x \in dom(f)\\
      \infty, & other\ wise\\
    \end{cases}\\
  \end{align*}
  if $f(x)$ convex, then $\tilde{f}$ is also convex\\

  Sublevel set of a function
  \begin{align*}
    C(\alpha) = \{ x \in dom(f): f(x) \leq \alpha \}
  \end{align*}
  For convex function, all sublevel sets are convex ($\forall \alpha$). Converse is not true.\\
  
  Quasi-convex function: if its sublevel sets are all convex.\\
  
  Epigraph of functions:\\
  $epi(f)=\{(x,t): x \in dom(f), f(x) \leq t\} \in \R^{n+1}, \\f \in \R^n \to \R$.\\
  
 $f$ is convex function $\iff epi(f)$ is convex set\\

 \subsection{First order condition}
 Suppose f is differentiable and domain of f is convex. Then:\\
 f convex $\iff \\ (\forall x,x_0 \in dom(f)) f(x) \geq f(x_0) + \nabla f(x_0)^T(x-x_0)$\\

  rough proof:\\
  suppose $f(x)$ is convex but $(\exists x, x_0) f(x) < f(x) + \nabla f(x_0)^T(x-x_0)$\\
  then this means the function should bend across the tangent line which violates the convexity\\
  
  proof for converse direction:\\
  suppose that $(\exists x, x_0) f(x) \geq f(x) + \nabla f(x_0)^T(x-x_0)$\\
  to show that $f(x)$ is convex lets take $x,y \in dom(f), z= \theta x + (1-\theta)y$\\
  $\theta f(x) + (1-\theta) f(y) \geq f(z) + \nabla f(z)^T(\theta x - \theta z + (1-\theta)y - (1-\theta)z)$\\
  $\theta f(x) + (1-\theta) f(y) \geq f(\theta x +(1-\theta)y)$\\
  $f(x)$ is convex
  \subsection{Second order condition}
  Suppose $f$ is twice differentiable and $dom(f)$ is convex, \\
  then $f(x)$ is convex $\iff \nabla^2 f(x) \geq 0 $ (PSD, eg: wrt. $S_+^n$)\\
  
  proof for scalar case:\\
  suppose that $f(x)$ is convex, then the first-order condition holds\\
  for $x,y \in dom(f): f(x) \geq f(y) + f'(y)(x-y)$\\
  for $y,x \in dom(f): f(y) \geq f(x) + f'(x)(y-x)$\\
  $f'(x)(y-x) \leq f(y)-f(x) \leq f'(y)(y-x)$\\
  $f'(x)(y-x) \leq f'(y)(y-x) \implies 0 \leq (y-x)(f'(x)-f'(y))$\\
  % $\frac{x}{(y-x)^2} \implies 0 \leq \frac{f;(y)-f'(x)}{(y-x)}$
  take $y\to x: 0 \leq f''(x)$\\
  $f''(x) \geq \frac{f'(x+\delta x)-f'(x)}{\delta x}$\\

  conversely, suppose that $f'(z) \geq 0, \forall z \in dom(f)$, take $x,y \in dom(f)$ WLOG $x < y$\\
  $\int_x^y f''(z)(y-z) dz \geq 0$\\
  $f''(z) \geq 0, (y-z) \geq 0 = I_1+I_2$\\
  $I_1 = \int_x^y f''(z)y dz- y f'(z)|_x^y = y(f'(y)-f'(x))$\\
  $I_2 = -\int_x^y f''(z) dz$\\
  $dv=f''(z) dz \implies v = f''(z)$\\
  $u=z\implies du = dz$\\
  $I_2 = -z f'(z)|_x^y + \int_x^y f(z) dz = -y f'(y) + x f'(x)+f(y)-f(x)$\\
  $I_1+I_2=y f'(y)-y f'(x)-y f'(y) + x f'(x) + f(y)-f(x) \geq 0$\\
  $\implies f(y) \geq f(x) + f'(x)(y-x)$ first order condition: $x<y$\\
  first order condition holds $\implies f(x)$ convex\\

  \vfill\null
  \columnbreak

  \subsection{Inequalities}
  $x \preceq_K y \iff y-x \in K$\\
  
  $x$ is a minimum in $S$ wrt. cone $K$:
  \begin{align*}
    &x \in S: (\forall y \in S)f(y) \succeq_K f(x) \iff f(y)-f(x) \in S\\
    &S \subseteq x+K
  \end{align*}
  $x$ is a minimal in $S$ wrt. cone $K$:
  \begin{align*}
    &x \in S: (\forall y \in S)f(y) \preceq_K f(x) \implies x = y\\
    &x \in S: (\forall y \in S) f(x)-f(y) \in K \implies x = y\\
    &(x-K) \cap S = \set{x}
  \end{align*}
  \subsection{Cone}
  \begin{align*}
    (\forall x \in C, \forall \theta \geq 0) \theta x \in C
  \end{align*}
  \subsection{Convex Cone}
  Eg: $S^n, S^n_+$ are convex cones\\
  convexity check for $S^N_+$:\\
  \begin{align*}
    x_1 \in S^n_+ \implies v^Tx_1v \geq 0\\
    x_2 \in S^n_+ \implies v^Tx_2v \geq 0\\
    v^T(\theta x_1 + (1-\theta)x_2)v \geq 0\\
    v^T\theta x_1 v + (1-\theta)v^T x_2 v \implies x \in S^n_+\\
  \end{align*}
  convexity check for cone:\\
  \begin{align*}
    x_1 \in S^n_+ \implies \theta x_1 \in S^n_+, \theta \geq 0\\
    (\forall v) v^T x v \geq 0 \implies v^T(\theta x) v \geq 0 \implies \text{ cone}
  \end{align*}
  
  \subsection{Proper Cone}
  Definition:
  \begin{itemize}
  \item convex
  \item closed(contains all limit points)
  \item solid(non-empty interior)
  \item pointed(contains no line): $x \in K \implies -x \not\in K$
  \end{itemize}
  Then the proper cone K defines a generalized inequality ($\leq_K$) in $\R^n$
  \begin{align*}
    x \leq_K y \implies y-x \in K\\
    x <_K y \implies y-x \in int(K)
  \end{align*}
  Example: $K=R^n_+$ (non-negative orthant):
  \begin{align*}
    n = 2\\
    x \leq_{R^2_+} y \implies y-x \in R^2_+
  \end{align*}
  Cone provides partial ordering using difference of 2 objects.\\
  $X \leq_{S^n_+} Y \iff Y-X \in S^n_+ \iff Y-X$ is PSD

  \subsection{Norm Cone}
  \begin{align*}
    K=\set{(x,t) \in \R^{n+1} : \norm{x} \leq t}, x \in \R^n
  \end{align*}
  
  \subsection{Dual norm}
  $\|z\|_* := \sup_x \{z^T x : \|x\|_p \leq 1\}$\\
  
  Dual of L1-norm:\\
  $\|z\|_* := \sup_x \{z^T x : \|x\|_1 \leq 1\}$\\
  max $\sum_i z_i x_i$,\\
  subject to : $\sum_i \|x_i\| \leq 1$\\
  select $x_i$ corresponding to $z_i$ with maximum absolute value\\
  equivalent to $\|z\|_* = \|z\|_{\infty}$\\

  Dual of L-$\infty$-norm:\\
  $\|z\|_* := \sup_x \{z^T x : \|x\|_{\infty} \leq 1\}$\\
  max $\sum_i z_i x_i$,\\
  subject to : $\|x_i\| \leq 1, \forall i$\\
  choose $x_i=1$ if $z_i \geq 0$ and $x_i=0$ if $z_i < 0$\\
  equivalent to $\|z\|_* = \|z\|_1$\\

  Dual norm of Lp-norm: Lq-norm where $1/p + 1/q = 1$\\

  Properties:
  \begin{itemize}
  \item $K^*$ closed and convex
  \item $K_1 \subseteq K_2 \implies k_2^* \subseteq K_1^*$
  \item $K$ has non-empty interior $\implies K^*$ pointed
  \item $cl(K)$ pointed $\implies K^*$ has non-tempty interior
  \item $K^{**} = cl(convhull(K))$
  \item $K$ convex and closed $\implies K=K^{**}$
  \end{itemize}
  
  \subsection{Operator norm}
  $\norm{X}_{a,b}=sup\{\norm{Xu}_a : \norm{u}_b \leq 1 \}, X \in \R^{m\times n}$
  
  \subsection{Dual cone}
  \begin{align*}
    K\ is\ a\ cone\\
    K^* = \{y:x^T y \geq 0, \forall x \in K\}
  \end{align*}

  \subsection{Dual norm cone}

  \begin{align*}
    K^* = \set{(u,v): \norm{u}_* \leq v}\\
    where\ K = \set{(x,t): \norm{x} \leq t}
  \end{align*}
  
  % $\norm{u}_* = sup \set{ u^Tx: \norm{x} \leq 1}$\\
    
  % interpreted as norm of $u^T$\\
  % from dual norm definition:\\
  % $z^T x \leq \norm{x} \norm{z}_*$\\
  % can be tightened: given x, $(\exists z) z^T x = \norm{x} \norm{z}_*$\\
  % given z, $(\exists x) z^T x = \norm{x} \norm{z}_*$\\
  
  \subsection{support function of a set}
  \begin{align*}
    S_C(x) = \sup \set{x^T y : y \in C}\\
    dom(S_C) = \set{x: \sup_{y\in C} c^Ty < \infty}
  \end{align*}
  It is pointwise supremum of convex function, so it is convex.
  
  \vfill\null
    
  \pagebreak
  
  \section{Appendix}

  \subsection{Gradient of Log Det}
  $f(x)=log(det X)$\\
  \begin{align*}
    f(X+\delta X) &= logdet(X+\delta X)\\
                  &= logdet((X^{\frac{1}{2}}(I+X^{-\frac{1}{2}}) \delta X X^{-\frac{1}{2}})X^{\frac{1}{2}})\\
                  &= logdet(X)+logdet(I+X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}})\\
                  & \text{let } M = X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}}\\
                  &= logdet(X)+logdet(I+M)
  \end{align*}
  claim eigenvalues of $I+M$: $1+\lambda_i$
  \begin{align*}
    Mv_i &= \lambda_i v_i\\
    (I+M)v_i &= (1+\lambda_i)v_i\\
    det(M)&=\prod_i(1+\lambda_i)\\
    f(X+\delta X) &= logdet(X) + log \prod_i(1+\lambda_i)\\
         &= logdet(X) + \sum_i log(1+\lambda_i)\\
         &\approx logdet(X) + \sum_i \lambda_i \text{ since } \delta X \text{ is small}\\
         &\approx logdet(X) + trace(X^{-\frac{1}{2}} \delta X X^{\frac{1}{2}})\\
         &\approx logdet(X) + trace(X^{-1} \delta X)\\
    trace(X^{-1} \delta X) &= (X^{-T})^T \delta X\\
    f(X+\delta X) &= f(X) + (\nabla f(X))^T \delta X \implies \nabla f(X) = X^{-1}\\
    logdet(X) &= log(X) \implies f'(X)=\frac{1}{X}
  \end{align*}
    
  \subsection{2nd order approximation of Log Det}
  \begin{align*}
    f(X+\delta X) = f(X) + <\nabla f(X), \delta X>  + 1/2 <\delta X, \nabla^2 f(x) \delta X>
  \end{align*}
  first look at first order approximation: $g(X) = X^{-1}$
  \begin{align*}
    g(X+\delta X) &= (X+\delta X)^{-1} = (X^{\frac{1}{2}}(I+X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}}) X^{\frac{1}{2}})^{-1}\\
                  &= X^{-\frac{1}{2}}(I+X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}})^{-1} X^{-\frac{1}{2}}\\
                  &\text{for small A(small eigenvalues): } (I+A)^{-1} \approx I-A\\
                  &= X^{-\frac{1}{2}}(I-X^{-\frac{1}{2}} \delta X X^{-\frac{1}{2}}) X^{-\frac{1}{2}}\\
                  &= X^{-1} - X^{-1} \delta X X^{-1}
  \end{align*}
  \begin{align*}
    logdet(X+\delta X)&=logdet(X)+tr(X^{-1}\delta X) - \frac{1}{2} tr(\delta X X^{-1} \delta X X^{-1})
  \end{align*}
  
  \pagebreak
  
  \section{Miscellaneuous Properties}
  $(a+x)^{-1} \approx 1-x$\\
  $\lim_{t \to 0} \frac{f(x+\epsilon t) - f(x)}{t} = \ppartial{f(x)}{x} \epsilon$\\
  
\end{multicols*}

\end {document}
